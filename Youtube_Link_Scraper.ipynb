{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad534dfb-120d-408b-84e2-c36defa85d19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "\n",
    "def scrape_kworb_with_direct_links():\n",
    "    videos = []\n",
    "    url = 'https://kworb.net/youtube/topvideos.html'\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        table = soup.find('table')\n",
    "        if not table:\n",
    "            print(\"Could not find table\")\n",
    "            return videos\n",
    "        \n",
    "        rows = table.find_all('tr')        \n",
    "        rank = 1\n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            \n",
    "            if len(cells) != 3:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                title_cell = cells[0]\n",
    "                title = title_cell.get_text(strip=True)\n",
    "                views = cells[1].get_text(strip=True)\n",
    "                \n",
    "                if not title or len(title) < 3:\n",
    "                    continue\n",
    "                \n",
    "                youtube_link = None\n",
    "                link_elem = title_cell.find('a')\n",
    "                \n",
    "                if link_elem:\n",
    "                    href = link_elem.get('href', '')\n",
    "                    \n",
    "                    if rank <= 3:\n",
    "                        print(f\"Row {rank}: Title='{title[:50]}'\")\n",
    "                        print(f\"  href='{href}'\")\n",
    "                    \n",
    "                    if href:\n",
    "                        if 'youtube.com' in href:\n",
    "                            youtube_link = href if href.startswith('http') else 'https://' + href\n",
    "                        elif 'youtu.be' in href:\n",
    "                            youtube_link = href if href.startswith('http') else 'https://' + href\n",
    "                        else:\n",
    "                            vid_match = re.search(r'([a-zA-Z0-9_-]{11})', href)\n",
    "                            if vid_match:\n",
    "                                video_id = vid_match.group(1)\n",
    "                                youtube_link = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "                                if rank <= 3:\n",
    "                                    print(f\"  Extracted video ID: {video_id}\")\n",
    "                \n",
    "                views_num = None\n",
    "                try:\n",
    "                    views_clean = views.replace(',', '')\n",
    "                    views_num = int(float(views_clean))\n",
    "                except:\n",
    "                    views_num = views\n",
    "                \n",
    "                video_info = {\n",
    "                    'rank': rank,\n",
    "                    'title': title,\n",
    "                    'views': views,\n",
    "                    'views_num': views_num,\n",
    "                    'youtube_url': youtube_link\n",
    "                }\n",
    "                \n",
    "                videos.append(video_info)\n",
    "                rank += 1\n",
    "                \n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        return videos\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return videos\n",
    "\n",
    "\n",
    "def save_to_csv(videos, csv_filename='top_videos_kworb.csv'):\n",
    "    if not videos:\n",
    "        print(\"No videos to save\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(videos)\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "\n",
    "\n",
    "def extract_urls_to_file(csv_filename='top_videos_kworb.csv', output_filename='video_urls.txt'):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_filename)\n",
    "        urls = df['youtube_url'].dropna().tolist()\n",
    "        \n",
    "        urls = [url for url in urls if url != 'nan' and isinstance(url, str) and url.startswith('http')]\n",
    "        \n",
    "        with open(output_filename, 'w') as f:\n",
    "            for url in urls:\n",
    "                f.write(url + '\\n')\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    videos = scrape_kworb_with_direct_links()\n",
    "    \n",
    "    if videos:\n",
    "        save_to_csv(videos, 'top_videos_kworb.csv')\n",
    "        extract_urls_to_file('top_videos_kworb.csv', 'video_urls.txt')\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to scrape videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9d49bb-506d-4c61-b6cc-8541eede7835",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_kworb_with_direct_links():\n",
    "    videos = []\n",
    "    url = 'https://kworb.net/youtube/topvideos.html'\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=15)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        table = soup.find('table')\n",
    "        if not table:\n",
    "            print(\"Could not find table\")\n",
    "            return videos\n",
    "        \n",
    "        rows = table.find_all('tr')        \n",
    "        rank = 1\n",
    "        for row in rows:\n",
    "            cells = row.find_all('td')\n",
    "            \n",
    "            if len(cells) != 3:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                title_cell = cells[0]\n",
    "                title = title_cell.get_text(strip=True)\n",
    "                views = cells[1].get_text(strip=True)\n",
    "                \n",
    "                if not title or len(title) < 3:\n",
    "                    continue\n",
    "                \n",
    "                youtube_link = None\n",
    "                link_elem = title_cell.find('a')\n",
    "                \n",
    "                if link_elem:\n",
    "                    href = link_elem.get('href', '')\n",
    "                    \n",
    "                    if rank <= 3:\n",
    "                        print(f\"Row {rank}: Title='{title[:50]}'\")\n",
    "                        print(f\"  href='{href}'\")\n",
    "                    \n",
    "                    if href:\n",
    "                        if 'youtube.com' in href:\n",
    "                            youtube_link = href if href.startswith('http') else 'https://' + href\n",
    "                        elif 'youtu.be' in href:\n",
    "                            youtube_link = href if href.startswith('http') else 'https://' + href\n",
    "                        else:\n",
    "                            vid_match = re.search(r'([a-zA-Z0-9_-]{11})', href)\n",
    "                            if vid_match:\n",
    "                                video_id = vid_match.group(1)\n",
    "                                youtube_link = f\"https://www.youtube.com/watch?v={video_id}\"\n",
    "                                if rank <= 3:\n",
    "                                    print(f\"  Extracted video ID: {video_id}\")\n",
    "                \n",
    "                views_num = None\n",
    "                try:\n",
    "                    views_clean = views.replace(',', '')\n",
    "                    views_num = int(float(views_clean))\n",
    "                except:\n",
    "                    views_num = views\n",
    "                \n",
    "                video_info = {\n",
    "                    'rank': rank,\n",
    "                    'title': title,\n",
    "                    'views': views,\n",
    "                    'views_num': views_num,\n",
    "                    'youtube_url': youtube_link\n",
    "                }\n",
    "                \n",
    "                videos.append(video_info)\n",
    "                rank += 1\n",
    "                \n",
    "                \n",
    "            except Exception as e:\n",
    "                continue\n",
    "        \n",
    "        return videos\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return videos\n",
    "\n",
    "\n",
    "def save_to_csv(videos, csv_filename='top_videos_kworb.csv'):\n",
    "    if not videos:\n",
    "        print(\"No videos to save\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(videos)\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "\n",
    "\n",
    "def extract_urls_to_file(csv_filename='top_videos_kworb.csv', output_filename='video_urls.txt'):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_filename)\n",
    "        urls = df['youtube_url'].dropna().tolist()\n",
    "        \n",
    "        urls = [url for url in urls if url != 'nan' and isinstance(url, str) and url.startswith('http')]\n",
    "        \n",
    "        with open(output_filename, 'w') as f:\n",
    "            for url in urls:\n",
    "                f.write(url + '\\n')\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    videos = scrape_kworb_with_direct_links()\n",
    "    \n",
    "    if videos:\n",
    "        save_to_csv(videos, 'top_videos_kworb.csv')\n",
    "        extract_urls_to_file('top_videos_kworb.csv', 'video_urls.txt')\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to scrape videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9ce0ea-d5a9-4a14-9876-06a5c3858cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(videos, csv_filename='top_videos_kworb.csv'):\n",
    "    if not videos:\n",
    "        print(\"No videos to save\")\n",
    "        return\n",
    "    \n",
    "    df = pd.DataFrame(videos)\n",
    "    df.to_csv(csv_filename, index=False)\n",
    "\n",
    "\n",
    "def extract_urls_to_file(csv_filename='top_videos_kworb.csv', output_filename='video_urls.txt'):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_filename)\n",
    "        urls = df['youtube_url'].dropna().tolist()\n",
    "        \n",
    "        urls = [url for url in urls if url != 'nan' and isinstance(url, str) and url.startswith('http')]\n",
    "        \n",
    "        with open(output_filename, 'w') as f:\n",
    "            for url in urls:\n",
    "                f.write(url + '\\n')\n",
    "\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385f1ced-f209-4ea8-8bea-08a2ac5684fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    videos = scrape_kworb_with_direct_links()\n",
    "    \n",
    "    if videos:\n",
    "        save_to_csv(videos, 'top_videos_kworb.csv')\n",
    "        extract_urls_to_file('top_videos_kworb.csv', 'video_urls.txt')\n",
    "\n",
    "    else:\n",
    "        print(\"Failed to scrape videos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(ML_Project)",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
